{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "261c7c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952e316c",
   "metadata": {},
   "source": [
    "### 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d8a6dfaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loaded 100,000 ratings\n",
      "‚úì Loaded 1,682 movies\n"
     ]
    }
   ],
   "source": [
    "ratings = pd.read_csv('data/processed/processed_ratings.csv')\n",
    "movies = pd.read_csv('data/processed/processed_movies.csv')\n",
    "\n",
    "print(f\"‚úì Loaded {len(ratings):,} ratings\")\n",
    "print(f\"‚úì Loaded {len(movies):,} movies\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eba9714",
   "metadata": {},
   "source": [
    "### 2. User Item Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a99d1d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Matrix shape: (943, 1682)\n",
      "‚úì Sparsity: 93.70%\n"
     ]
    }
   ],
   "source": [
    "user_item_matrix = ratings.pivot_table(\n",
    "    index='user_id',\n",
    "    columns='item_id',\n",
    "    values='rating',\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "print(f\"‚úì Matrix shape: {user_item_matrix.shape}\")\n",
    "print(f\"‚úì Sparsity: {(1 - (ratings.shape[0] / (user_item_matrix.shape[0] * user_item_matrix.shape[1])))*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fd151d",
   "metadata": {},
   "source": [
    "### 3. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4a535565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Training set: 80,000 ratings\n",
      "‚úì Test set: 20,000 ratings\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data = train_test_split(ratings, test_size=0.2, stratify=ratings['user_id'], random_state=42)\n",
    "print(f\"‚úì Training set: {len(train_data):,} ratings\")\n",
    "print(f\"‚úì Test set: {len(test_data):,} ratings\")\n",
    "\n",
    "# Create train matrix\n",
    "train_matrix = train_data.pivot_table(\n",
    "    index='user_id',\n",
    "    columns='item_id',\n",
    "    values='rating',\n",
    "    fill_value=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6e02b2",
   "metadata": {},
   "source": [
    "### 4. Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "85626e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "def calculate_metrics(predictions, test_data, movies_df, k=10):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive evaluation metrics: Precision, Recall, NDCG, and Coverage.\n",
    "    \n",
    "    Args:\n",
    "        predictions (dict): Dictionary mapping user_id to a list of recommended item_ids.\n",
    "        test_data (pd.DataFrame): The test dataset containing actual user ratings.\n",
    "        movies_df (pd.DataFrame): The movies dataset (used for coverage calculation).\n",
    "        k (int): Number of top recommendations to evaluate.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary containing the calculated metrics.\n",
    "    \"\"\"\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    ndcgs = []\n",
    "    \n",
    "    # OPTIMIZATION: Pre-group test data by user to avoid slow filtering inside the loop\n",
    "    # This converts the DataFrame into a dictionary {user_id: set(item_ids)}\n",
    "    actual_user_items = test_data.groupby('user_id')['item_id'].apply(set).to_dict()\n",
    "    \n",
    "    for user_id, actual_items in actual_user_items.items():\n",
    "        # We only evaluate users who actually received predictions\n",
    "        if user_id in predictions and len(actual_items) > 0:\n",
    "            \n",
    "            # Get top-k predictions\n",
    "            # Note: We use a list here to preserve order for NDCG calculation\n",
    "            pred_list = predictions[user_id][:k]\n",
    "            \n",
    "            if not pred_list:\n",
    "                continue\n",
    "                \n",
    "            # --- 1. Identify Hits ---\n",
    "            # Create a binary list (1 if item is relevant, 0 if not)\n",
    "            hits = [1 if item in actual_items else 0 for item in pred_list]\n",
    "            num_hits = sum(hits)\n",
    "            \n",
    "            # --- 2. Precision@K ---\n",
    "            # (Relevant Items Found) / K\n",
    "            precisions.append(num_hits / k)\n",
    "            \n",
    "            # --- 3. Recall@K ---\n",
    "            # (Relevant Items Found) / (Total Relevant Items in Test Set)\n",
    "            recalls.append(num_hits / len(actual_items))\n",
    "            \n",
    "            # --- 4. NDCG@K (Ranking Quality) ---\n",
    "            # Calculate DCG: sum(rel_i / log2(rank_i + 1))\n",
    "            # We use idx+2 because enumerate starts at 0, and log formula uses rank (1-based) + 1\n",
    "            dcg = sum([rel / np.log2(idx + 2) for idx, rel in enumerate(hits)])\n",
    "            \n",
    "            # Calculate IDCG: Ideal DCG (if all hits were at the very top)\n",
    "            possible_hits = min(len(actual_items), k)\n",
    "            idcg = sum([1.0 / np.log2(idx + 2) for idx in range(possible_hits)])\n",
    "            \n",
    "            if idcg > 0:\n",
    "                ndcgs.append(dcg / idcg)\n",
    "            else:\n",
    "                ndcgs.append(0)\n",
    "\n",
    "    # --- 5. Coverage ---\n",
    "    # Count unique items recommended across all users\n",
    "    all_recommended_items = set()\n",
    "    for recs in predictions.values():\n",
    "        all_recommended_items.update(recs[:k])\n",
    "    \n",
    "    total_movies = movies_df['movie_id'].nunique()\n",
    "    coverage = len(all_recommended_items) / total_movies if total_movies > 0 else 0\n",
    "\n",
    "    return {\n",
    "        f'Precision@{k}': np.mean(precisions) if precisions else 0.0,\n",
    "        f'Recall@{k}': np.mean(recalls) if recalls else 0.0,\n",
    "        f'NDCG@{k}': np.mean(ndcgs) if ndcgs else 0.0,\n",
    "        'Coverage': coverage,\n",
    "        'Avg_Recs_Per_User': np.mean([len(v) for v in predictions.values()])\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facfeb1e",
   "metadata": {},
   "source": [
    "### 5. User Based Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e851a7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Training time: 72.28 seconds\n",
      "‚úì Precision@10:  0.3203\n",
      "‚úì Recall@10:     0.2095\n",
      "‚úì NDCG@10:       0.3837\n",
      "‚úì Coverage:      0.2004\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Calculate user similarity\n",
    "user_similarity = cosine_similarity(train_matrix)\n",
    "user_similarity_df = pd.DataFrame(\n",
    "    user_similarity,\n",
    "    index=train_matrix.index,\n",
    "    columns=train_matrix.index\n",
    ")\n",
    "\n",
    "def user_based_recommendations(user_id, n=10, n_neighbors=20):\n",
    "    \"\"\"Generate recommendations using user-based CF\"\"\"\n",
    "    if user_id not in user_similarity_df.index:\n",
    "        return []\n",
    "    \n",
    "    # Get similar users\n",
    "    similar_users = user_similarity_df[user_id].sort_values(ascending=False)[1:n_neighbors+1]\n",
    "    \n",
    "    # Get items the user hasn't rated\n",
    "    user_items = set(train_data[train_data['user_id'] == user_id]['item_id'].values)\n",
    "    \n",
    "    # Score items based on similar users' ratings\n",
    "    item_scores = {}\n",
    "    for sim_user, similarity in similar_users.items():\n",
    "        sim_user_items = train_data[train_data['user_id'] == sim_user]\n",
    "        for _, row in sim_user_items.iterrows():\n",
    "            if row['item_id'] not in user_items:\n",
    "                if row['item_id'] not in item_scores:\n",
    "                    item_scores[row['item_id']] = 0\n",
    "                item_scores[row['item_id']] += similarity * row['rating']\n",
    "    \n",
    "    # Sort and return top N\n",
    "    recommendations = sorted(item_scores.items(), key=lambda x: x[1], reverse=True)[:n]\n",
    "    return [item_id for item_id, score in recommendations]\n",
    "\n",
    "# Generate predictions for test users\n",
    "user_based_predictions = {}\n",
    "test_users = test_data['user_id'].unique()  \n",
    "for user_id in test_users:\n",
    "    user_based_predictions[user_id] = user_based_recommendations(user_id)\n",
    "\n",
    "training_time_ub = time.time() - start_time\n",
    "metrics_ub = calculate_metrics(user_based_predictions, test_data, movies)\n",
    "\n",
    "print(f\"‚úì Training time: {training_time_ub:.2f} seconds\")\n",
    "print(f\"‚úì Precision@10:  {metrics_ub['Precision@10']:.4f}\")\n",
    "print(f\"‚úì Recall@10:     {metrics_ub['Recall@10']:.4f}\")\n",
    "print(f\"‚úì NDCG@10:       {metrics_ub['NDCG@10']:.4f}\")\n",
    "print(f\"‚úì Coverage:      {metrics_ub['Coverage']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b4b58b",
   "metadata": {},
   "source": [
    "### 6. Item Based Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d5170f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Training time: 52.75 seconds\n",
      "‚úì Precision@10:  0.2908\n",
      "‚úì Recall@10:     0.1871\n",
      "‚úì NDCG@10:       0.3514\n",
      "‚úì Coverage:      0.1153\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Calculate item similarity\n",
    "item_similarity = cosine_similarity(train_matrix.T)\n",
    "item_similarity_df = pd.DataFrame(\n",
    "    item_similarity,\n",
    "    index=train_matrix.columns,\n",
    "    columns=train_matrix.columns\n",
    ")\n",
    "\n",
    "def item_based_recommendations(user_id, n=10):\n",
    "    \"\"\"Generate recommendations using item-based CF\"\"\"\n",
    "    # Get items the user has rated\n",
    "    user_items = train_data[train_data['user_id'] == user_id]\n",
    "    \n",
    "    if len(user_items) == 0:\n",
    "        return []\n",
    "    \n",
    "    # Score items based on similarity to user's rated items\n",
    "    item_scores = {}\n",
    "    for _, row in user_items.iterrows():\n",
    "        item_id = row['item_id']\n",
    "        if item_id in item_similarity_df.index:\n",
    "            similar_items = item_similarity_df[item_id].sort_values(ascending=False)[1:51]\n",
    "            \n",
    "            for sim_item, similarity in similar_items.items():\n",
    "                if sim_item not in user_items['item_id'].values:\n",
    "                    if sim_item not in item_scores:\n",
    "                        item_scores[sim_item] = 0\n",
    "                    item_scores[sim_item] += similarity * row['rating']\n",
    "    \n",
    "    # Sort and return top N\n",
    "    recommendations = sorted(item_scores.items(), key=lambda x: x[1], reverse=True)[:n]\n",
    "    return [item_id for item_id, score in recommendations]\n",
    "\n",
    "# Generate predictions\n",
    "item_based_predictions = {}\n",
    "for user_id in test_users:\n",
    "    item_based_predictions[user_id] = item_based_recommendations(user_id)\n",
    "\n",
    "training_time_ib = time.time() - start_time\n",
    "metrics_ib = calculate_metrics(item_based_predictions, test_data, movies)\n",
    "\n",
    "print(f\"‚úì Training time: {training_time_ib:.2f} seconds\")\n",
    "print(f\"‚úì Precision@10:  {metrics_ib['Precision@10']:.4f}\")\n",
    "print(f\"‚úì Recall@10:     {metrics_ib['Recall@10']:.4f}\")\n",
    "print(f\"‚úì NDCG@10:       {metrics_ib['NDCG@10']:.4f}\")\n",
    "print(f\"‚úì Coverage:      {metrics_ib['Coverage']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415907b2",
   "metadata": {},
   "source": [
    "### 7. SVD Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "13486498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Training time: 0.64 seconds\n",
      "‚úì Precision@10:  0.3060\n",
      "‚úì Recall@10:     0.2078\n",
      "‚úì NDCG@10:       0.3694\n",
      "‚úì Coverage:      0.2705\n",
      "‚úì Explained variance: 0.4639\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Apply SVD\n",
    "n_factors = 50\n",
    "svd = TruncatedSVD(n_components=n_factors, random_state=42)\n",
    "user_factors = svd.fit_transform(train_matrix)\n",
    "item_factors = svd.components_.T\n",
    "\n",
    "# Reconstruct rating matrix\n",
    "predicted_ratings = np.dot(user_factors, item_factors.T)\n",
    "predicted_ratings_df = pd.DataFrame(\n",
    "    predicted_ratings,\n",
    "    index=train_matrix.index,\n",
    "    columns=train_matrix.columns\n",
    ")\n",
    "\n",
    "def svd_recommendations(user_id, n=10):\n",
    "    \"\"\"Generate recommendations using SVD\"\"\"\n",
    "    if user_id not in predicted_ratings_df.index:\n",
    "        return []\n",
    "    \n",
    "    # Get user's predictions\n",
    "    user_predictions = predicted_ratings_df.loc[user_id]\n",
    "    \n",
    "    # Remove already rated items\n",
    "    user_rated = set(train_data[train_data['user_id'] == user_id]['item_id'].values)\n",
    "    user_predictions = user_predictions[~user_predictions.index.isin(user_rated)]\n",
    "    \n",
    "    # Return top N\n",
    "    recommendations = user_predictions.sort_values(ascending=False).head(n)\n",
    "    return recommendations.index.tolist()\n",
    "\n",
    "# Generate predictions\n",
    "svd_predictions = {}\n",
    "for user_id in test_users:\n",
    "    svd_predictions[user_id] = svd_recommendations(user_id)\n",
    "\n",
    "training_time_svd = time.time() - start_time\n",
    "metrics_svd = calculate_metrics(svd_predictions, test_data, movies)\n",
    "\n",
    "print(f\"‚úì Training time: {training_time_svd:.2f} seconds\")\n",
    "print(f\"‚úì Precision@10:  {metrics_svd['Precision@10']:.4f}\")\n",
    "print(f\"‚úì Recall@10:     {metrics_svd['Recall@10']:.4f}\")\n",
    "print(f\"‚úì NDCG@10:       {metrics_svd['NDCG@10']:.4f}\")\n",
    "print(f\"‚úì Coverage:      {metrics_svd['Coverage']:.4f}\")\n",
    "print(f\"‚úì Explained variance: {svd.explained_variance_ratio_.sum():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49907fc",
   "metadata": {},
   "source": [
    "### 8. Hybrid (Item Based + SVD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "81f0f30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Training time: 52.56 seconds\n",
      "‚úì Precision@10:  0.3284\n",
      "‚úì Recall@10:     0.2167\n",
      "‚úì NDCG@10:       0.3908\n",
      "‚úì Coverage:      0.2122\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "def hybrid_recommendations(user_id, n=10, weight_ib=0.5, weight_svd=0.5):\n",
    "    \"\"\"Combine item-based and SVD recommendations\"\"\"\n",
    "    ib_recs = item_based_recommendations(user_id, n=20)\n",
    "    svd_recs = svd_recommendations(user_id, n=20)\n",
    "    \n",
    "    # Combine scores\n",
    "    item_scores = {}\n",
    "    for i, item_id in enumerate(ib_recs):\n",
    "        item_scores[item_id] = weight_ib * (20 - i)\n",
    "    \n",
    "    for i, item_id in enumerate(svd_recs):\n",
    "        if item_id in item_scores:\n",
    "            item_scores[item_id] += weight_svd * (20 - i)\n",
    "        else:\n",
    "            item_scores[item_id] = weight_svd * (20 - i)\n",
    "    \n",
    "    # Sort and return top N\n",
    "    recommendations = sorted(item_scores.items(), key=lambda x: x[1], reverse=True)[:n]\n",
    "    return [item_id for item_id, score in recommendations]\n",
    "\n",
    "# Generate predictions\n",
    "hybrid_predictions = {}\n",
    "for user_id in test_users:\n",
    "    hybrid_predictions[user_id] = hybrid_recommendations(user_id)\n",
    "\n",
    "training_time_hybrid = time.time() - start_time\n",
    "metrics_hybrid = calculate_metrics(hybrid_predictions, test_data, movies)\n",
    "\n",
    "print(f\"‚úì Training time: {training_time_hybrid:.2f} seconds\")\n",
    "print(f\"‚úì Precision@10:  {metrics_hybrid['Precision@10']:.4f}\")\n",
    "print(f\"‚úì Recall@10:     {metrics_hybrid['Recall@10']:.4f}\")\n",
    "print(f\"‚úì NDCG@10:       {metrics_hybrid['NDCG@10']:.4f}\")\n",
    "print(f\"‚úì Coverage:      {metrics_hybrid['Coverage']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043d852d",
   "metadata": {},
   "source": [
    "### 9. Model Comparision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "acfe6fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Model  Precision@10  Recall@10  NDCG@10  Coverage  Training Time (s)\n",
      "User-Based CF      0.320255   0.209530 0.383700  0.200357          72.284236\n",
      "Item-Based CF      0.290774   0.187137 0.351412  0.115339          52.749058\n",
      "     SVD (MF)      0.306045   0.207758 0.369437  0.270511           0.640822\n",
      "       Hybrid      0.328420   0.216749 0.390792  0.212247          52.561404\n",
      "\n",
      "üèÜ BEST MODEL: Hybrid\n",
      "   Precision@10: 0.3284\n",
      "   Recall@10:    0.2167\n",
      "   NDCG@10:      0.3908\n"
     ]
    }
   ],
   "source": [
    "# Create comparison dataframe with ALL metrics\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['User-Based CF', 'Item-Based CF', 'SVD (MF)', 'Hybrid'],\n",
    "    \n",
    "    'Precision@10': [\n",
    "        metrics_ub['Precision@10'],\n",
    "        metrics_ib['Precision@10'],\n",
    "        metrics_svd['Precision@10'],\n",
    "        metrics_hybrid['Precision@10']\n",
    "    ],\n",
    "    \n",
    "    'Recall@10': [\n",
    "        metrics_ub['Recall@10'],\n",
    "        metrics_ib['Recall@10'],\n",
    "        metrics_svd['Recall@10'],\n",
    "        metrics_hybrid['Recall@10']\n",
    "    ],\n",
    "    \n",
    "    'NDCG@10': [\n",
    "        metrics_ub['NDCG@10'],\n",
    "        metrics_ib['NDCG@10'],\n",
    "        metrics_svd['NDCG@10'],\n",
    "        metrics_hybrid['NDCG@10']\n",
    "    ],\n",
    "    \n",
    "    'Coverage': [\n",
    "        metrics_ub['Coverage'],\n",
    "        metrics_ib['Coverage'],\n",
    "        metrics_svd['Coverage'],\n",
    "        metrics_hybrid['Coverage']\n",
    "    ],\n",
    "    \n",
    "    'Training Time (s)': [\n",
    "        training_time_ub,\n",
    "        training_time_ib,\n",
    "        training_time_svd,\n",
    "        training_time_hybrid\n",
    "    ]\n",
    "})\n",
    "\n",
    "# formatting for cleaner output\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Select best model (You can also change this to use 'NDCG@10' for better ranking quality)\n",
    "best_model_metric = 'Precision@10'  # or 'NDCG@10'\n",
    "best_model_idx = comparison_df[best_model_metric].idxmax()\n",
    "best_model = comparison_df.iloc[best_model_idx]['Model']\n",
    "\n",
    "print(f\"\\nüèÜ BEST MODEL: {best_model}\")\n",
    "print(f\"   {best_model_metric}: {comparison_df.iloc[best_model_idx][best_model_metric]:.4f}\")\n",
    "print(f\"   Recall@10:    {comparison_df.iloc[best_model_idx]['Recall@10']:.4f}\")\n",
    "print(f\"   NDCG@10:      {comparison_df.iloc[best_model_idx]['NDCG@10']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebccfbd",
   "metadata": {},
   "source": [
    "### 10. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "26d50ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Saved recommendation_models.pkl\n",
      "‚úì Saved model_comparison.csv\n",
      "\n",
      "================================================================================\n",
      "TRAINING COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "Best performing model: Hybrid\n",
      "Models are ready for deployment in Streamlit app!\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    'item_similarity': item_similarity_df,\n",
    "    'svd_model': svd,\n",
    "    'user_factors': user_factors,\n",
    "    'item_factors': item_factors,\n",
    "    'train_matrix': train_matrix,\n",
    "    'movies': movies,\n",
    "    'ratings': ratings\n",
    "}\n",
    "\n",
    "with open('recommendation_models.pkl', 'wb') as f:\n",
    "    pickle.dump(models, f)\n",
    "\n",
    "# Save comparison results\n",
    "comparison_df.to_csv('model_comparison.csv', index=False)\n",
    "\n",
    "print(\"‚úì Saved recommendation_models.pkl\")\n",
    "print(\"‚úì Saved model_comparison.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nBest performing model: {best_model}\")\n",
    "print(\"Models are ready for deployment in Streamlit app!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
