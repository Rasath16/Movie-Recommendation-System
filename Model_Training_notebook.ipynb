{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "261c7c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952e316c",
   "metadata": {},
   "source": [
    "### 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8a6dfaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded 100,000 ratings\n",
      "✓ Loaded 1,682 movies\n"
     ]
    }
   ],
   "source": [
    "ratings = pd.read_csv('data/processed/processed_ratings.csv')\n",
    "movies = pd.read_csv('data/processed/processed_movies.csv')\n",
    "\n",
    "print(f\"✓ Loaded {len(ratings):,} ratings\")\n",
    "print(f\"✓ Loaded {len(movies):,} movies\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eba9714",
   "metadata": {},
   "source": [
    "### 2. User Item Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a99d1d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Matrix shape: (943, 1682)\n",
      "✓ Sparsity: 93.70%\n"
     ]
    }
   ],
   "source": [
    "user_item_matrix = ratings.pivot_table(\n",
    "    index='user_id',\n",
    "    columns='item_id',\n",
    "    values='rating',\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "print(f\"✓ Matrix shape: {user_item_matrix.shape}\")\n",
    "print(f\"✓ Sparsity: {(1 - (ratings.shape[0] / (user_item_matrix.shape[0] * user_item_matrix.shape[1])))*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fd151d",
   "metadata": {},
   "source": [
    "### 3. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a535565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Training set: 80,000 ratings\n",
      "✓ Test set: 20,000 ratings\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = train_test_split(ratings, test_size=0.2, random_state=42)\n",
    "print(f\"✓ Training set: {len(train_data):,} ratings\")\n",
    "print(f\"✓ Test set: {len(test_data):,} ratings\")\n",
    "\n",
    "# Create train matrix\n",
    "train_matrix = train_data.pivot_table(\n",
    "    index='user_id',\n",
    "    columns='item_id',\n",
    "    values='rating',\n",
    "    fill_value=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6e02b2",
   "metadata": {},
   "source": [
    "### 4. Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85626e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(predictions, k=10):\n",
    "    \"\"\"Calculate Precision@K\"\"\"\n",
    "    precisions = []\n",
    "    \n",
    "    for user_id in test_data['user_id'].unique():\n",
    "        # Get actual rated items by user in test set\n",
    "        actual = set(test_data[test_data['user_id'] == user_id]['item_id'].values)\n",
    "        \n",
    "        if user_id in predictions and len(actual) > 0:\n",
    "            # Get top-k predicted items\n",
    "            predicted = set(predictions[user_id][:k])\n",
    "            \n",
    "            # Calculate precision\n",
    "            if len(predicted) > 0:\n",
    "                precision = len(actual & predicted) / k\n",
    "                precisions.append(precision)\n",
    "    \n",
    "    return np.mean(precisions) if precisions else 0\n",
    "\n",
    "def calculate_metrics(predictions, k=10):\n",
    "    \"\"\"Calculate multiple metrics\"\"\"\n",
    "    # Precision@K\n",
    "    precision = precision_at_k(predictions, k)\n",
    "    \n",
    "    # Coverage (what % of items can be recommended)\n",
    "    all_recommended = set()\n",
    "    for recs in predictions.values():\n",
    "        all_recommended.update(recs[:k])\n",
    "    coverage = len(all_recommended) / movies['movie_id'].nunique()\n",
    "    \n",
    "    return {\n",
    "        'precision@10': precision,\n",
    "        'coverage': coverage,\n",
    "        'avg_recommendations': np.mean([len(v) for v in predictions.values()])\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facfeb1e",
   "metadata": {},
   "source": [
    "### 5. User Based Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e851a7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Training time: 11.70 seconds\n",
      "✓ Precision@10: 0.4760\n",
      "✓ Coverage: 0.1207\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Calculate user similarity\n",
    "user_similarity = cosine_similarity(train_matrix)\n",
    "user_similarity_df = pd.DataFrame(\n",
    "    user_similarity,\n",
    "    index=train_matrix.index,\n",
    "    columns=train_matrix.index\n",
    ")\n",
    "\n",
    "def user_based_recommendations(user_id, n=10, n_neighbors=20):\n",
    "    \"\"\"Generate recommendations using user-based CF\"\"\"\n",
    "    if user_id not in user_similarity_df.index:\n",
    "        return []\n",
    "    \n",
    "    # Get similar users\n",
    "    similar_users = user_similarity_df[user_id].sort_values(ascending=False)[1:n_neighbors+1]\n",
    "    \n",
    "    # Get items the user hasn't rated\n",
    "    user_items = set(train_data[train_data['user_id'] == user_id]['item_id'].values)\n",
    "    \n",
    "    # Score items based on similar users' ratings\n",
    "    item_scores = {}\n",
    "    for sim_user, similarity in similar_users.items():\n",
    "        sim_user_items = train_data[train_data['user_id'] == sim_user]\n",
    "        for _, row in sim_user_items.iterrows():\n",
    "            if row['item_id'] not in user_items:\n",
    "                if row['item_id'] not in item_scores:\n",
    "                    item_scores[row['item_id']] = 0\n",
    "                item_scores[row['item_id']] += similarity * row['rating']\n",
    "    \n",
    "    # Sort and return top N\n",
    "    recommendations = sorted(item_scores.items(), key=lambda x: x[1], reverse=True)[:n]\n",
    "    return [item_id for item_id, score in recommendations]\n",
    "\n",
    "# Generate predictions for test users\n",
    "user_based_predictions = {}\n",
    "test_users = test_data['user_id'].unique()[:100]  # Sample for faster evaluation\n",
    "for user_id in test_users:\n",
    "    user_based_predictions[user_id] = user_based_recommendations(user_id)\n",
    "\n",
    "training_time_ub = time.time() - start_time\n",
    "metrics_ub = calculate_metrics(user_based_predictions)\n",
    "\n",
    "print(f\"✓ Training time: {training_time_ub:.2f} seconds\")\n",
    "print(f\"✓ Precision@10: {metrics_ub['precision@10']:.4f}\")\n",
    "print(f\"✓ Coverage: {metrics_ub['coverage']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b4b58b",
   "metadata": {},
   "source": [
    "### 6. Item Based Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5170f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Training time: 10.12 seconds\n",
      "✓ Precision@10: 0.4670\n",
      "✓ Coverage: 0.0773\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Calculate item similarity\n",
    "item_similarity = cosine_similarity(train_matrix.T)\n",
    "item_similarity_df = pd.DataFrame(\n",
    "    item_similarity,\n",
    "    index=train_matrix.columns,\n",
    "    columns=train_matrix.columns\n",
    ")\n",
    "\n",
    "def item_based_recommendations(user_id, n=10):\n",
    "    \"\"\"Generate recommendations using item-based CF\"\"\"\n",
    "    # Get items the user has rated\n",
    "    user_items = train_data[train_data['user_id'] == user_id]\n",
    "    \n",
    "    if len(user_items) == 0:\n",
    "        return []\n",
    "    \n",
    "    # Score items based on similarity to user's rated items\n",
    "    item_scores = {}\n",
    "    for _, row in user_items.iterrows():\n",
    "        item_id = row['item_id']\n",
    "        if item_id in item_similarity_df.index:\n",
    "            similar_items = item_similarity_df[item_id].sort_values(ascending=False)[1:51]\n",
    "            \n",
    "            for sim_item, similarity in similar_items.items():\n",
    "                if sim_item not in user_items['item_id'].values:\n",
    "                    if sim_item not in item_scores:\n",
    "                        item_scores[sim_item] = 0\n",
    "                    item_scores[sim_item] += similarity * row['rating']\n",
    "    \n",
    "    # Sort and return top N\n",
    "    recommendations = sorted(item_scores.items(), key=lambda x: x[1], reverse=True)[:n]\n",
    "    return [item_id for item_id, score in recommendations]\n",
    "\n",
    "# Generate predictions\n",
    "item_based_predictions = {}\n",
    "for user_id in test_users:\n",
    "    item_based_predictions[user_id] = item_based_recommendations(user_id)\n",
    "\n",
    "training_time_ib = time.time() - start_time\n",
    "metrics_ib = calculate_metrics(item_based_predictions)\n",
    "\n",
    "print(f\"✓ Training time: {training_time_ib:.2f} seconds\")\n",
    "print(f\"✓ Precision@10: {metrics_ib['precision@10']:.4f}\")\n",
    "print(f\"✓ Coverage: {metrics_ib['coverage']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415907b2",
   "metadata": {},
   "source": [
    "### 7. SVD Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13486498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Training time: 0.25 seconds\n",
      "✓ Precision@10: 0.4380\n",
      "✓ Coverage: 0.2039\n",
      "✓ Explained variance: 0.4645\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Apply SVD\n",
    "n_factors = 50\n",
    "svd = TruncatedSVD(n_components=n_factors, random_state=42)\n",
    "user_factors = svd.fit_transform(train_matrix)\n",
    "item_factors = svd.components_.T\n",
    "\n",
    "# Reconstruct rating matrix\n",
    "predicted_ratings = np.dot(user_factors, item_factors.T)\n",
    "predicted_ratings_df = pd.DataFrame(\n",
    "    predicted_ratings,\n",
    "    index=train_matrix.index,\n",
    "    columns=train_matrix.columns\n",
    ")\n",
    "\n",
    "def svd_recommendations(user_id, n=10):\n",
    "    \"\"\"Generate recommendations using SVD\"\"\"\n",
    "    if user_id not in predicted_ratings_df.index:\n",
    "        return []\n",
    "    \n",
    "    # Get user's predictions\n",
    "    user_predictions = predicted_ratings_df.loc[user_id]\n",
    "    \n",
    "    # Remove already rated items\n",
    "    user_rated = set(train_data[train_data['user_id'] == user_id]['item_id'].values)\n",
    "    user_predictions = user_predictions[~user_predictions.index.isin(user_rated)]\n",
    "    \n",
    "    # Return top N\n",
    "    recommendations = user_predictions.sort_values(ascending=False).head(n)\n",
    "    return recommendations.index.tolist()\n",
    "\n",
    "# Generate predictions\n",
    "svd_predictions = {}\n",
    "for user_id in test_users:\n",
    "    svd_predictions[user_id] = svd_recommendations(user_id)\n",
    "\n",
    "training_time_svd = time.time() - start_time\n",
    "metrics_svd = calculate_metrics(svd_predictions)\n",
    "\n",
    "print(f\"✓ Training time: {training_time_svd:.2f} seconds\")\n",
    "print(f\"✓ Precision@10: {metrics_svd['precision@10']:.4f}\")\n",
    "print(f\"✓ Coverage: {metrics_svd['coverage']:.4f}\")\n",
    "print(f\"✓ Explained variance: {svd.explained_variance_ratio_.sum():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49907fc",
   "metadata": {},
   "source": [
    "### 8. Hybrid (Item Based + SVD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81f0f30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Training time: 10.11 seconds\n",
      "✓ Precision@10: 0.5000\n",
      "✓ Coverage: 0.1427\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "def hybrid_recommendations(user_id, n=10, weight_ib=0.5, weight_svd=0.5):\n",
    "    \"\"\"Combine item-based and SVD recommendations\"\"\"\n",
    "    ib_recs = item_based_recommendations(user_id, n=20)\n",
    "    svd_recs = svd_recommendations(user_id, n=20)\n",
    "    \n",
    "    # Combine scores\n",
    "    item_scores = {}\n",
    "    for i, item_id in enumerate(ib_recs):\n",
    "        item_scores[item_id] = weight_ib * (20 - i)\n",
    "    \n",
    "    for i, item_id in enumerate(svd_recs):\n",
    "        if item_id in item_scores:\n",
    "            item_scores[item_id] += weight_svd * (20 - i)\n",
    "        else:\n",
    "            item_scores[item_id] = weight_svd * (20 - i)\n",
    "    \n",
    "    # Sort and return top N\n",
    "    recommendations = sorted(item_scores.items(), key=lambda x: x[1], reverse=True)[:n]\n",
    "    return [item_id for item_id, score in recommendations]\n",
    "\n",
    "# Generate predictions\n",
    "hybrid_predictions = {}\n",
    "for user_id in test_users:\n",
    "    hybrid_predictions[user_id] = hybrid_recommendations(user_id)\n",
    "\n",
    "training_time_hybrid = time.time() - start_time\n",
    "metrics_hybrid = calculate_metrics(hybrid_predictions)\n",
    "\n",
    "print(f\"✓ Training time: {training_time_hybrid:.2f} seconds\")\n",
    "print(f\"✓ Precision@10: {metrics_hybrid['precision@10']:.4f}\")\n",
    "print(f\"✓ Coverage: {metrics_hybrid['coverage']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043d852d",
   "metadata": {},
   "source": [
    "### 9. Model Comparision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acfe6fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Model  Precision@10  Coverage  Training Time (s)\n",
      "User-Based CF         0.476  0.120690          11.704220\n",
      "Item-Based CF         0.467  0.077289          10.123723\n",
      "     SVD (MF)         0.438  0.203924           0.252365\n",
      "       Hybrid         0.500  0.142687          10.110859\n",
      "\n",
      "🏆 BEST MODEL: Hybrid\n",
      "   Precision@10: 0.5000\n"
     ]
    }
   ],
   "source": [
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['User-Based CF', 'Item-Based CF', 'SVD (MF)', 'Hybrid'],\n",
    "    'Precision@10': [\n",
    "        metrics_ub['precision@10'],\n",
    "        metrics_ib['precision@10'],\n",
    "        metrics_svd['precision@10'],\n",
    "        metrics_hybrid['precision@10']\n",
    "    ],\n",
    "    'Coverage': [\n",
    "        metrics_ub['coverage'],\n",
    "        metrics_ib['coverage'],\n",
    "        metrics_svd['coverage'],\n",
    "        metrics_hybrid['coverage']\n",
    "    ],\n",
    "    'Training Time (s)': [\n",
    "        training_time_ub,\n",
    "        training_time_ib,\n",
    "        training_time_svd,\n",
    "        training_time_hybrid\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Select best model\n",
    "best_model_idx = comparison_df['Precision@10'].idxmax()\n",
    "best_model = comparison_df.iloc[best_model_idx]['Model']\n",
    "\n",
    "print(f\"\\n🏆 BEST MODEL: {best_model}\")\n",
    "print(f\"   Precision@10: {comparison_df.iloc[best_model_idx]['Precision@10']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebccfbd",
   "metadata": {},
   "source": [
    "### 10. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26d50ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved recommendation_models.pkl\n",
      "✓ Saved model_comparison.csv\n",
      "\n",
      "================================================================================\n",
      "TRAINING COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "Best performing model: Hybrid\n",
      "Models are ready for deployment in Streamlit app!\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    'item_similarity': item_similarity_df,\n",
    "    'svd_model': svd,\n",
    "    'user_factors': user_factors,\n",
    "    'item_factors': item_factors,\n",
    "    'train_matrix': train_matrix,\n",
    "    'movies': movies,\n",
    "    'ratings': ratings\n",
    "}\n",
    "\n",
    "with open('recommendation_models.pkl', 'wb') as f:\n",
    "    pickle.dump(models, f)\n",
    "\n",
    "# Save comparison results\n",
    "comparison_df.to_csv('model_comparison.csv', index=False)\n",
    "\n",
    "print(\"✓ Saved recommendation_models.pkl\")\n",
    "print(\"✓ Saved model_comparison.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nBest performing model: {best_model}\")\n",
    "print(\"Models are ready for deployment in Streamlit app!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
